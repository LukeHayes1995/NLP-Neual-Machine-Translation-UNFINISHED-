{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2_Luke.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYQt0nUEbAgN"
      },
      "source": [
        "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system with an attention model.\n",
        "\n",
        "This is an **individual assignment** and usual rules for plagiarism apply! With this you agree that: \"In submitting this work I confirm that it is entirely my own. I acknowledge that I may be invited to online interview if there is any concern in relation to the integrity of my exam.\" \n",
        "\n",
        "**Write comments and documentation.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33hukl0w-Nh"
      },
      "source": [
        "## Section 1- Data Collection and Preprocessing \n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAFJXKQaeN-d"
      },
      "source": [
        "**Task 1  (5 marks)**\n",
        "\n",
        "---\n",
        "\n",
        "There are few datasets to train an NMT system available from the OPUS project (http://opus.nlpl.eu/).\n",
        "\n",
        "*  Download a language pair (preferably European language) and **extract** the file(s) and upload it to CoLab\n",
        "*  Create a list of lines by splitting the text file at every occurrence accordingly, i.e. source and target language\n",
        "*  Print number of sentences\n",
        "*  Limit the number of sentences to 10,000 lines (but more than 5,000 lines)\n",
        "*  Split the data into train, development and test set\n",
        "*  Print 100th sentence in original script for source and target language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRRto303mA6j"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJx_0F0bmA6k"
      },
      "source": [
        "latent_dim = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9BaH3ozUGf"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "german_sentences = []\n",
        "english_sentences = []\n",
        "\n",
        "num_sentences = 10000\n",
        "#english_file = open(\"C:/Users/lhaye/Documents/NLP_Assignment_2/CCAligned.de-en.txt\", encoding=\"utf8\")\n",
        "#german_file = open(\"C:/Users/lhaye/Documents/NLP_Assignment_2/CCAligned.de-en.txt\", encoding=\"utf8\")\n",
        "\n",
        "with open(\"C:/Users/lhaye/Documents/NLP_Assignment_2/GNOME.de-en_en.txt\", encoding=\"utf8\") as english_file:\n",
        "    for i in range(num_sentences):    \n",
        "        english_sentences.append(english_file.readline())\n",
        "        \n",
        "with open(\"C:/Users/lhaye/Documents/NLP_Assignment_2/GNOME.de-en_de.txt\", encoding=\"utf8\") as german_file:\n",
        "    for i in range(num_sentences):    \n",
        "        german_sentences.append(german_file.readline())\n",
        "\n",
        "#english_file.close()\n",
        "#german_file.close()\n",
        "\n",
        "#print(english_sentences[100])\n",
        "#print(german_sentences[100])\n",
        "\n",
        "#NOW WE NEED TO SPLIT THE DATA INTO TRAIN, DEVELOPMENT AND TEST\n",
        "#LETS SAY 70, 15, 15\n",
        "\n",
        "data_frame = {'English':english_sentences,'German':german_sentences}\n",
        "data_frame = pd.DataFrame(data_frame)\n",
        "\n",
        "train, test = train_test_split(data_frame, test_size=0.3)\n",
        "val, test_final = train_test_split(test, test_size=0.5)\n",
        "\n",
        "#print(train['English'])\n",
        "#print(train['German'][2])\n",
        "\n",
        "#print(train['English'])\n",
        "#x=train['English'] \n",
        "\n",
        "english_train_sent_list = train['English'].to_list()\n",
        "english_test_sent_list = test_final['English'].to_list()\n",
        "\n",
        "german_train_sent_list = train['German'].to_list()\n",
        "german_test_sent_list = test_final['German'].to_list()\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "    i += 1\n",
        "    #print(i)\n",
        "    #print(english_sent_list[i])\n",
        "    #print(german_sent_list[i])\n",
        "\n",
        "#NOW WE HAVE ALL THE DATA \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x63IEWjUxkJj"
      },
      "source": [
        "**Task 2 (5 marks)** \n",
        "\n",
        "---\n",
        "\n",
        "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
        "* Perform the pre-processing step of the text.\n",
        "* Print the last 5 sentences of the preprocessed text.\n",
        "* Print statistics on the selected dataset:\n",
        "  * Number of samples\n",
        "  * Number of unique source language tokens\n",
        "  * Number of unique target language tokens\n",
        "  * Max sequence length of source language\n",
        "  * Max sequence length of target language\n",
        "  * Source Vocabulary\n",
        "  * Target Vocabulary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjkGuJCMmA6m"
      },
      "source": [
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re \n",
        "\n",
        "#OKAY SO LETS DO IT ON THE TRAINING DATA \n",
        "english_text_list = []\n",
        "german_text_list_input = []\n",
        "german_text_list_output = []\n",
        "english_text_list_nottok = []\n",
        "german_input_text_list_nottok = []\n",
        "german_output_text_list_nottok = []\n",
        "#WE USE SETS BECAUSE IT DOESN'T DO DUPLICATES\n",
        "english_words = set()\n",
        "german_words = set()\n",
        "english_words_dict = {}\n",
        "german_words_dict = {}\n",
        "\n",
        "\n",
        "num_samples_train = 7000\n",
        "for sent_idx in range(num_samples_train):\n",
        "    \n",
        "    #GET THE ENGLISH AND GERMAN SENTENCE FOR THE INDEX\n",
        "    english_sentence = english_train_sent_list[sent_idx]\n",
        "    german_sentence_input = german_train_sent_list[sent_idx]\n",
        "    german_sentence_output = german_train_sent_list[sent_idx]\n",
        "\n",
        "    #SET THE WORDS TO BE LOWER CASE\n",
        "    english_sentence = english_sentence.lower()\n",
        "    german_sentence_input = german_sentence_input.lower()\n",
        "    \n",
        "    #print(english_sentence)\n",
        "    \n",
        "    #NOW REMOVE NUMBERS\n",
        "    english_sentence = re.sub(r'\\d+', '', english_sentence)\n",
        "    german_sentence_input = re.sub(r'\\d+', '', german_sentence_input)\n",
        "    german_sentence_output = re.sub(r'\\d+', '', german_sentence_input)\n",
        "\n",
        "\n",
        "    #NOW WE NEED TO ADD THE '<bof>' TO THE START AND '<eof>' TO THE END OF THE GERMAN TEXT \n",
        "    #german_sentence_input.insert(0, '<bof>')\n",
        "    #german_sentence_tok_output.append('<eof>')\n",
        "    \n",
        "    german_sentence_input = '<sos> ' + german_sentence_input\n",
        "    german_sentence_output = german_sentence_output + ' <eos>'\n",
        "    \n",
        "    #english_sentence = [word for word in english_sentence if word.isalnum()]\n",
        "    #german_sentence_input = [word for word in german_sentence_input if word.isalnum()]\n",
        "    #german_sentence_output = [word for word in german_sentence_output if word.isalnum()]\n",
        "    \n",
        "    print(english_sentence)\n",
        "    \n",
        "    english_sentence = english_sentence.replace('\\n', '')\n",
        "    german_sentence_input = german_sentence_input.replace('\\n', '')\n",
        "    german_sentence_output = german_sentence_output.replace('\\n', '')\n",
        "\n",
        "    \n",
        "    #WE SHOULD ONLY BE WORKING WITH THESE \n",
        "    english_text_list_nottok.append(english_sentence)\n",
        "    german_input_text_list_nottok.append(german_sentence_input)\n",
        "    german_output_text_list_nottok.append(german_sentence_output)\n",
        "\n",
        "    #TOKENIZE THE WORDS IN THE SENTENCE \n",
        "    #english_sentence_tok = word_tokenize(english_sentence)\n",
        "    #german_sentence_tok_input = word_tokenize(german_sentence_input)\n",
        "    #german_sentence_tok_output = word_tokenize(german_sentence_output)\n",
        "\n",
        "    #REMOVE ALL WORDS/TOKENS THAT DO NOT CONTAIN CHARACTERS\n",
        "    #english_sentence_tok = [word for word in english_sentence_tok if word.isalnum()]\n",
        "    #german_sentence_tok_input = [word for word in german_sentence_tok_input if word.isalnum()]\n",
        "    #german_sentence_tok_output = [word for word in german_sentence_tok_output if word.isalnum()]\n",
        "\n",
        "    #german_sentence_tok_input = '<bof>' + german_sentence_tok_input\n",
        "    #german_sentence_tok_output = german_sentence_tok_output + '<eof>'\n",
        "    \n",
        "    #WE NEED TO DO THE SAME HERE EXCEPT WITH A DICT \n",
        "    #{word: occurances}\n",
        "    #for word in english_sentence_tok:\n",
        "        #if word not in english_words_dict:\n",
        "          #english_words_dict[word] = 1\n",
        "        #else:\n",
        "            #english_words_dict[word] += 1\n",
        "    #for word in german_sentence_tok_input:\n",
        "        #if word not in german_words_dict:\n",
        "          #german_words_dict[word] = 1\n",
        "        #else:\n",
        "            #german_words_dict[word] += 1\n",
        "       \n",
        "\n",
        "    #PRINT THE LAST 5 \n",
        "    #if (sent_idx > (num_samples_train-5) and (sent_idx <= num_samples_train)):\n",
        "        #print(english_sentence_tok)\n",
        "        #print(german_sentence_tok_input)\n",
        "        #print(german_sentence_tok_output)\n",
        "        \n",
        "    #ADD THE SENTENCE TO A LIST\n",
        "    #english_text_list.append(english_sentence_tok)\n",
        "    #german_text_list_input.append(german_sentence_tok_input)\n",
        "    #german_text_list_output.append(german_sentence_tok_output)\n",
        "\n",
        "    #CHECK IF THE WORD IS ALREADY IN THE LIST OF UNIQUE WORDS AND ADD IF NOT\n",
        "    #for word in english_text_list_nottok:\n",
        "        #if word not in english_words:\n",
        "          #english_words.add(word)\n",
        "    #for word in german_sentence_tok_input:\n",
        "        #if word not in german_words:\n",
        "          #german_words.add(word)\n",
        "    #for word in german_sentence_tok_output:\n",
        "        #if word not in german_words:\n",
        "          #german_words.add(word)      \n",
        "#print(english_words_dict['computer']) \n",
        "#print(german_words) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YR3ZPbUmA6r"
      },
      "source": [
        "#WE SHOULD ONLY BE WORKING WITH THESE \n",
        "#english_text_list_nottok\n",
        "#german_input_text_list_nottok\n",
        "#german_output_text_list_nottok\n",
        "\n",
        "num_samples = len(english_text_list_nottok)\n",
        "#print(num_samples)\n",
        "MAX_NUM_WORDS = 50000\n",
        "#print(german_input_text_list_nottok)\n",
        "#print(german_output_text_list_nottok)\n",
        "#print(english_text_list_nottok)\n",
        "\n",
        "print(english_text_list_nottok[8])\n",
        "print(german_output_text_list_nottok[8])\n",
        "print(german_input_text_list_nottok[8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JjZQRmKmA6s"
      },
      "source": [
        "print(german_output_text_list_nottok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsUF82hmA6t"
      },
      "source": [
        "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "input_tokenizer.fit_on_texts(english_text_list_nottok)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(english_text_list_nottok)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Length of longest sentence in input: %g\" % max_input_len)\n",
        "\n",
        "print(word2idx_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMkvDesHmA6t"
      },
      "source": [
        "print(input_integer_seq[172])\n",
        "print(english_text_list_nottok[172])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6sFgr2mA6u"
      },
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "output_tokenizer.fit_on_texts(german_output_text_list_nottok + german_input_text_list_nottok)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(german_output_text_list_nottok)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(german_input_text_list_nottok)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
        "\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Length of longest sentence in the output: %g\" % max_out_len)\n",
        "\n",
        "print(output_input_integer_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LSG3MNumA6v",
        "outputId": "7a6c1234-f800-4511-bfc7-1049f87cf6b1"
      },
      "source": [
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
        "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])\n",
        "print(english_train_sent_list[172])\n",
        "print(input_integer_seq[172])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_input_sequences.shape: (7000, 125)\n",
            "encoder_input_sequences[172]: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 221 108]\n",
            "Quick Select\n",
            "\n",
            "[221, 108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hme1eogSmA6w",
        "outputId": "9f376d24-8144-402e-aa99-31c9257e0561"
      },
      "source": [
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
        "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_input_sequences.shape: (7000, 124)\n",
            "decoder_input_sequences[172]: [  2 962   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofzJYMpRmA6w",
        "outputId": "960ae956-57b8-4b7f-a695-88dbe6620694"
      },
      "source": [
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences.shape:\", decoder_output_sequences.shape)\n",
        "print(\"decoder_input_sequences[172]:\", decoder_output_sequences[172])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_input_sequences.shape: (7000, 124)\n",
            "decoder_input_sequences[172]: [962   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deyu-ccPmA6x",
        "outputId": "c6e41170-f23e-43fe-923b-e18bcc365817"
      },
      "source": [
        "print(word2idx_outputs[\"<sos>\"])\n",
        "print(word2idx_outputs[\"<eos>\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxQeW4VUmA6y"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "\n",
        "glove_file = open('./glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcLJt3WFmA6z",
        "outputId": "16dbaa16-e4c8-4481-ac64-655f4c327ff5"
      },
      "source": [
        "#print(english_token_index)\n",
        "num_words = len(word2idx_inputs) + 1\n",
        "embedding_matrix = zeros((num_words, 100))\n",
        "for word, index in word2idx_inputs.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        \n",
        "print(embeddings_dictionary[\"computer\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.6298e-01  3.0141e-01  5.7978e-01  6.6548e-02  4.5835e-01 -1.5329e-01\n",
            "  4.3258e-01 -8.9215e-01  5.7747e-01  3.6375e-01  5.6524e-01 -5.6281e-01\n",
            "  3.5659e-01 -3.6096e-01 -9.9662e-02  5.2753e-01  3.8839e-01  9.6185e-01\n",
            "  1.8841e-01  3.0741e-01 -8.7842e-01 -3.2442e-01  1.1202e+00  7.5126e-02\n",
            "  4.2661e-01 -6.0651e-01 -1.3893e-01  4.7862e-02 -4.5158e-01  9.3723e-02\n",
            "  1.7463e-01  1.0962e+00 -1.0044e+00  6.3889e-02  3.8002e-01  2.1109e-01\n",
            " -6.6247e-01 -4.0736e-01  8.9442e-01 -6.0974e-01 -1.8577e-01 -1.9913e-01\n",
            " -6.9226e-01 -3.1806e-01 -7.8565e-01  2.3831e-01  1.2992e-01  8.7721e-02\n",
            "  4.3205e-01 -2.2662e-01  3.1549e-01 -3.1748e-01 -2.4632e-03  1.6615e-01\n",
            "  4.2358e-01 -1.8087e+00 -3.6699e-01  2.3949e-01  2.5458e+00  3.6111e-01\n",
            "  3.9486e-02  4.8607e-01 -3.6974e-01  5.7282e-02 -4.9317e-01  2.2765e-01\n",
            "  7.9966e-01  2.1428e-01  6.9811e-01  1.1262e+00 -1.3526e-01  7.1972e-01\n",
            " -9.9605e-04 -2.6842e-01 -8.3038e-01  2.1780e-01  3.4355e-01  3.7731e-01\n",
            " -4.0251e-01  3.3124e-01  1.2576e+00 -2.7196e-01 -8.6093e-01  9.0053e-02\n",
            " -2.4876e+00  4.5200e-01  6.6945e-01 -5.4648e-01 -1.0324e-01 -1.6979e-01\n",
            "  5.9437e-01  1.1280e+00  7.5755e-01 -5.9160e-02  1.5152e-01 -2.8388e-01\n",
            "  4.9452e-01 -9.1703e-01  9.1289e-01 -3.0927e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IAzq9EqmA6z",
        "outputId": "05d56970-cd56-4758-aaac-663e5152d024"
      },
      "source": [
        "#WE NEED TO FIX THOSE EMBEDDINGS \n",
        "embedding_layer = Embedding(num_words, 100, weights=[embedding_matrix], input_length=max_input_len)\n",
        "#embedding_layer = Embedding(num_english_tokens, 300, input_length=max_english_seq_length)\n",
        "\n",
        "print(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
            "   0.27061999]\n",
            " [-0.1529     -0.24279     0.89837003 ... -0.59100002  1.00390005\n",
            "   0.20664001]\n",
            " ...\n",
            " [ 0.07401     0.50411999  0.21244    ... -0.55091     0.42706999\n",
            "   0.30928001]\n",
            " [-0.96662003  0.64036     0.27232    ... -0.1137      0.51686001\n",
            "   0.40193999]\n",
            " [ 0.083247    0.41929999  0.35538    ...  0.42758    -0.29018\n",
            "   0.86532998]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqHC0Qj_mA60"
      },
      "source": [
        "NOW ONTO THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVuri0aGmA60"
      },
      "source": [
        "decoder_targets_one_hot = np.zeros((\n",
        "        len(english_text_list_nottok),\n",
        "        (max_out_len),\n",
        "        (num_words_output)\n",
        "    ),\n",
        "    dtype='float32'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjlndAdVmA60",
        "outputId": "20175741-a667-4e06-dcd7-22db2e1aecad"
      },
      "source": [
        "print(decoder_targets_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 124, 3596)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8hyhlzQmA61"
      },
      "source": [
        "for i, d in enumerate(decoder_output_sequences):\n",
        "    for t, word in enumerate(d):\n",
        "        decoder_targets_one_hot[i, t, word] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AtTtP6nmA61"
      },
      "source": [
        "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "encoder_states = [h, c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_lNPGV7mA61"
      },
      "source": [
        "decoder_inputs_placeholder = Input(shape=((max_out_len),))\n",
        "\n",
        "decoder_embedding = Embedding(num_words_output, latent_dim)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65GHCVlMmA61"
      },
      "source": [
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjLzJPQYmA62"
      },
      "source": [
        "model = Model([encoder_inputs_placeholder,\n",
        "  decoder_inputs_placeholder], decoder_outputs)\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJjvhTxFmA62"
      },
      "source": [
        "model.load_weights('./seq2seq_eng-de.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK7NOFtCmA62",
        "outputId": "2c8002ee-56b2-4dac-e5d0-e81c41d9c1a7"
      },
      "source": [
        "r = model.fit(\n",
        "    [encoder_input_sequences, decoder_input_sequences],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "99/99 [==============================] - 245s 2s/step - loss: 1.7255 - accuracy: 0.8727 - val_loss: 0.4993 - val_accuracy: 0.9239\n",
            "Epoch 2/20\n",
            "99/99 [==============================] - 228s 2s/step - loss: 0.4493 - accuracy: 0.9304 - val_loss: 0.4481 - val_accuracy: 0.9283\n",
            "Epoch 3/20\n",
            "99/99 [==============================] - 226s 2s/step - loss: 0.3983 - accuracy: 0.9348 - val_loss: 0.4026 - val_accuracy: 0.9330\n",
            "Epoch 4/20\n",
            "99/99 [==============================] - 217s 2s/step - loss: 0.3626 - accuracy: 0.9376 - val_loss: 0.3682 - val_accuracy: 0.9369\n",
            "Epoch 5/20\n",
            "99/99 [==============================] - 222s 2s/step - loss: 0.3238 - accuracy: 0.9421 - val_loss: 0.3332 - val_accuracy: 0.9418\n",
            "Epoch 6/20\n",
            "99/99 [==============================] - 217s 2s/step - loss: 0.2834 - accuracy: 0.9477 - val_loss: 0.3088 - val_accuracy: 0.9459\n",
            "Epoch 7/20\n",
            "99/99 [==============================] - 212s 2s/step - loss: 0.2593 - accuracy: 0.9507 - val_loss: 0.2757 - val_accuracy: 0.9501\n",
            "Epoch 8/20\n",
            "99/99 [==============================] - 214s 2s/step - loss: 0.2278 - accuracy: 0.9553 - val_loss: 0.2530 - val_accuracy: 0.9532\n",
            "Epoch 9/20\n",
            "99/99 [==============================] - 226s 2s/step - loss: 0.2097 - accuracy: 0.9589 - val_loss: 0.2298 - val_accuracy: 0.9581\n",
            "Epoch 10/20\n",
            "99/99 [==============================] - 220s 2s/step - loss: 0.1733 - accuracy: 0.9661 - val_loss: 0.2133 - val_accuracy: 0.9609\n",
            "Epoch 11/20\n",
            "99/99 [==============================] - 212s 2s/step - loss: 0.1546 - accuracy: 0.9700 - val_loss: 0.1934 - val_accuracy: 0.9636\n",
            "Epoch 12/20\n",
            "99/99 [==============================] - 215s 2s/step - loss: 0.1387 - accuracy: 0.9730 - val_loss: 0.1771 - val_accuracy: 0.9670\n",
            "Epoch 13/20\n",
            "99/99 [==============================] - 178s 2s/step - loss: 0.1204 - accuracy: 0.9767 - val_loss: 0.1651 - val_accuracy: 0.9692\n",
            "Epoch 14/20\n",
            "99/99 [==============================] - 192s 2s/step - loss: 0.1078 - accuracy: 0.9795 - val_loss: 0.1439 - val_accuracy: 0.9732\n",
            "Epoch 15/20\n",
            "99/99 [==============================] - 168s 2s/step - loss: 0.0913 - accuracy: 0.9833 - val_loss: 0.1363 - val_accuracy: 0.9744\n",
            "Epoch 16/20\n",
            "99/99 [==============================] - 186s 2s/step - loss: 0.0809 - accuracy: 0.9853 - val_loss: 0.1210 - val_accuracy: 0.9777\n",
            "Epoch 17/20\n",
            "99/99 [==============================] - 185s 2s/step - loss: 0.0697 - accuracy: 0.9878 - val_loss: 0.1099 - val_accuracy: 0.9802\n",
            "Epoch 18/20\n",
            "99/99 [==============================] - 188s 2s/step - loss: 0.0605 - accuracy: 0.9896 - val_loss: 0.1002 - val_accuracy: 0.9820\n",
            "Epoch 19/20\n",
            "99/99 [==============================] - 180s 2s/step - loss: 0.0512 - accuracy: 0.9914 - val_loss: 0.0917 - val_accuracy: 0.9842\n",
            "Epoch 20/20\n",
            "99/99 [==============================] - 171s 2s/step - loss: 0.0422 - accuracy: 0.9933 - val_loss: 0.0876 - val_accuracy: 0.9846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl4_408kmA63"
      },
      "source": [
        "model.save('./seq2seq_eng-de_new2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHaK_TlPmA63"
      },
      "source": [
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJntpamPmA63"
      },
      "source": [
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTF2cqofmA64"
      },
      "source": [
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y68QUqiQmA64"
      },
      "source": [
        "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9dh_VUvmA64"
      },
      "source": [
        "decoder_states = [h, c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNbuQFE1mA65"
      },
      "source": [
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZdRFcYmA65"
      },
      "source": [
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}\n",
        "print(idx2word_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K30QuN6_mA65"
      },
      "source": [
        "def translate_sentence(input_seq):\n",
        "    \n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "    output_sentence = []\n",
        "\n",
        "    for _ in range(max_input_len):\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "        \n",
        "        #print(idx)\n",
        "        #print(eos)\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        word = ''\n",
        "        \n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        target_seq[0, 0] = idx\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuxrgCXmA66",
        "outputId": "7fcecb18-cc6c-420a-aa0c-300546e4feb0"
      },
      "source": [
        "#i = np.random.choice(len(english_text_list_nottok))\n",
        "#print(english_text_list_nottok[i])\n",
        "#input_seq = encoder_input_sequences[i:i+1]\n",
        "#print(encoder_input_sequences[i:i+1])\n",
        "#translation = translate_sentence(input_seq)\n",
        "#print('-')\n",
        "#print('Input:', english_text_list_nottok[i])\n",
        "#print('output:', german_input_text_list_nottok[i])\n",
        "\n",
        "#print('Response:', translation)\n",
        "\n",
        "for i in range(10):\n",
        "    index = i + 1\n",
        "    input_seq = encoder_input_sequences[index:index+1]\n",
        "    #print(input_seq)\n",
        "    translation = translate_sentence(input_seq)\n",
        "    \n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(\"Sentence Number: \" , index)\n",
        "    print('English Input Sentence: ', english_text_list_nottok[index])\n",
        "    print('Actual German Translation: ' , german_input_text_list_nottok[index])\n",
        "    print('Perdicted Translation: ', translation)\n",
        "    #print('Response:', translation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------\n",
            "Sentence Number:  1\n",
            "English Input Sentence:  the border color of the highlight box\n",
            "Actual German Translation:  <sos> die randfarbe der hervorhebenden box\n",
            "Perdicted Translation:  die randfarbe der hervorhebenden box\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  2\n",
            "English Input Sentence:  unknown\n",
            "Actual German Translation:  <sos> unbekannt\n",
            "Perdicted Translation:  unbekannt\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  3\n",
            "English Input Sentence:  some of accerciser's elements, such as the global hotkey ctrl+alt+/, highlight a specific widget in a target application. the highlighting duration, border color, and fill color can be customized via the preferences dialog, under the \"highlighting\" tab.\n",
            "Actual German Translation:  <sos> ein paar von den elementen von accerciser, z.b. die globalen tastenkürzel strg+alt+/, markieren eine spezielle funktion einer zielanwendung. die dauer der markierung, die farbe des rahmens, und die füllfarbe können in den einstellung, unter dem reiter »hervorhebung«, verändert werden.\n",
            "Perdicted Translation:  ein paar von accerciser ist für den baumansicht stellt der baumansicht wird eine spezielle funktion einer anwendung von einem funktionen und die zielanwendung. die anwendung zur verfügung für die funktionen und in die funktionen und können.\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  4\n",
            "English Input Sentence:  thieves\n",
            "Actual German Translation:  <sos> diebe\n",
            "Perdicted Translation:  diebe\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  5\n",
            "English Input Sentence:  cards can be removed in pairs that add up to fourteen. aces are worth one and jacks, queens, and kings are worth , , and  respectively.\n",
            "Actual German Translation:  <sos> karten können in paaren entfernt werden, deren summe vierzehn ergibt. asse zählen eins, buben, damen und könige zählen jeweils ,  bzw. .\n",
            "Perdicted Translation:  karten können in paaren entfernt werden, deren summe vierzehn ergibt. asse zählen eins, buben, damen und könige zählen jeweils , bzw. .\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  6\n",
            "English Input Sentence:  ./autogen.sh # or ./configure for first time installation\n",
            "Actual German Translation:  <sos> ./autogen.sh # oder ./configure für die erstinstallation\n",
            "Perdicted Translation:  ./autogen.sh # oder ./configure für die erstinstallation\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  7\n",
            "English Input Sentence:  top panel\n",
            "Actual German Translation:  <sos> obere leiste\n",
            "Perdicted Translation:  obere leiste\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  8\n",
            "English Input Sentence:  gaps\n",
            "Actual German Translation:  <sos> gaps\n",
            "Perdicted Translation:  gaps\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  9\n",
            "English Input Sentence:  watch your step! try to move cards that are resting on matching pairs as otherwise you might get stuck.\n",
            "Actual German Translation:  <sos> seien sie vorsichtig! versuchen sie karten zu verschieben, die auf passenden paaren liegen, andernfalls kann es zu einer blockade kommen.\n",
            "Perdicted Translation:  klicken sie den talon, um sie zu karten in den passenden fundamentstapel, falls dies zu ist.\n",
            "---------------------------------------------------------------\n",
            "Sentence Number:  10\n",
            "English Input Sentence:  this is a diffcult game. try to find two or three cards of the same rank at or near the last row. try not to remove any card of this rank. at the end you can move these cards onto each other to win.\n",
            "Actual German Translation:  <sos> dies ist ein schwieriges spiel. versuchen sie, zwei oder drei karten gleichen ranges in oder nahe der letzten spalte zu finden. zum schluss können sie diese karten zusammenschieben, um zu gewinnen.\n",
            "Perdicted Translation:  dies ist ein schwieriges spiel. versuchen sie, zwei oder drei karten gleichen ranges oder oder nahe der letzten spalte zu finden. zum schluss können sie diese karten zusammenschieben, um zu gewinnen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cycMIiLOmA66",
        "outputId": "a335a0db-d01a-43e1-8c80-1c317e44a8a3"
      },
      "source": [
        "#OKAY SO NOW WE NEED TO \n",
        "#LOOP THROUGH THE LIST OF SENTENCES\n",
        "\n",
        "#print(len(english_test_sent_list))\n",
        "indexes = []\n",
        "for i in range(len(english_test_sent_list)):\n",
        "    \n",
        "    index = []\n",
        "    #WE NEED TO TOKENIZE THE INPUT AND REMOVE THE /N\n",
        "    \n",
        "    tokens = nltk.word_tokenize(english_test_sent_list[i])\n",
        "    tokens=[word.lower() for word in tokens if word.isalpha()]\n",
        "    \n",
        "    for token in tokens:\n",
        "        if token not in word2idx_inputs:\n",
        "            print(\"Word isn't in this\")\n",
        "        else:\n",
        "            idx = word2idx_inputs[token]\n",
        "            index.append(idx)\n",
        "                \n",
        "    indexes.append(index)\n",
        "    \n",
        "    #input_integer_seq = input_tokenizer.texts_to_sequences(english_text_list_nottok)\n",
        "encoder_input_sequences_new = pad_sequences(indexes, maxlen=max_input_len)\n",
        "print(encoder_input_sequences_new)\n",
        "\n",
        "translated_sentences = []\n",
        "actual_sentences = []\n",
        "\n",
        "for i in range(len(english_test_sent_list)):\n",
        "    translation = translate_sentence(encoder_input_sequences_new[i:i+1])\n",
        "    #print(\"---------------------------------------------------------------\")\n",
        "    #print(\"Sentence Number: \" , i)\n",
        "    #print('English Input Sentence: ', english_test_sent_list[i])\n",
        "    #print('Actual German Translation: ' , german_test_sent_list[i])\n",
        "    #print('Perdicted Translation: ', translation)\n",
        "    actual_sentences.append(german_test_sent_list[i])\n",
        "    translated_sentences.append(translation)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "Word isn't in this\n",
            "[[   0    0    0 ...    0    0 1361]\n",
            " [   0    0    0 ...    0    0 1633]\n",
            " [   0    0    0 ...    0    0  779]\n",
            " ...\n",
            " [   0    0    0 ...    7   21   99]\n",
            " [   0    0    0 ...    1 1932 1933]\n",
            " [   0    0    0 ...    0    0  835]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBV8giYHmA67",
        "outputId": "f0e173ca-401e-4ec8-d19a-6a4cf876c94d"
      },
      "source": [
        "print(translated_sentences[1494])\n",
        "print(actual_sentences[1494])\n",
        "print(english_test_sent_list[1494])\n",
        "#OKAY SO NOW WE NEED TO CALCULATE THE BLEU SCORE \n",
        "\n",
        "for i in range(len(actual_sentences)):\n",
        "    actual_sentences[i] = actual_sentences[i].lower()\n",
        "    \n",
        "print(actual_sentences[1494])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fokusobjekt %s hat eine tabellenschnittstelle, aber keine auswahlschnittstellebutton has focused state without focusable state\n",
            "Fokusobjekt %s hat eine Tabellenschnittstelle, aber keine Auswahlschnittstellebutton has focused state without focusable state\n",
            "\n",
            "focusable %s has a table interface, but not a selection interface\n",
            "\n",
            "fokusobjekt %s hat eine tabellenschnittstelle, aber keine auswahlschnittstellebutton has focused state without focusable state\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUvHZF3mmA67",
        "outputId": "5a7b9498-12ed-4376-ad32-bf3f8e5d2123"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "scores = 0.00\n",
        "num = 0\n",
        "\n",
        "for i in range(len(actual_sentences)):\n",
        "    \n",
        "    num += 1\n",
        "    \n",
        "    tokens_actual = actual_sentences[i].split()\n",
        "    tokens_translated = translated_sentences[i].split()\n",
        "   \n",
        "    tokens_actual = [tokens_actual]\n",
        "\n",
        "    score = sentence_bleu(tokens_actual, tokens_translated, weights=(1, 0, 0, 0))\n",
        "    scores += score\n",
        "    \n",
        "print(\"This is the BLEU score for the Test Set: \" , (scores/num))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the BLEU score for the Test Set:  0.7099551247027194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuQ7B78BhLrx"
      },
      "source": [
        "**Task 3 (5 marks)** \n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "*  Assign each unique word an integer value (5 marks).\n",
        "*  Create word embedding for your vocabulary using pre-trained embeddings, for example GloVe or fastText (10 marks) (https://nlp.stanford.edu/projects/glove/ , https://fasttext.cc/docs/en/english-vectors.html)\n",
        "* Print the first line of the embeddings (see below) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfLgKEgazro"
      },
      "source": [
        "## Section 2 Translation Model training\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8WnlX8d0RVj"
      },
      "source": [
        "**Task 4 (15 marks)**\n",
        "* Provide code for the encoder using Keras LSTM (5 marks)\n",
        "* Provide code for the decoder using Keras LSTM (5 marks)\n",
        "* Train the sequence2sequence (encoder-decoder) model (5 marks) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sy_WCp31x79"
      },
      "source": [
        "## Section 3 Testing\n",
        "\n",
        "---\n",
        "\n",
        "**Task 5 (20 marks)**\n",
        "\n",
        "* Use the trained model to translate the text from the source into the target language (10 marks). \n",
        "* Use the test/evaluation set (see Section 1) and perform an automatic evaluation with the BLEU metric (10 marks). \n",
        "You can use the NLTK library to calculate BLEU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb4F1-a00Hw6"
      },
      "source": [
        "# Section 4 Attention\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XTD-fCC1yUA"
      },
      "source": [
        "**Task 5 (40 Marks)** Sequence2Sequence\n",
        "\n",
        "* Extend the existing Seq2Seq model with an attention mechanism [Discussed in Class]\n",
        "* Create sequence2sequence model with attention (15 marks)\n",
        "* Train the model with the same data from Section 1 (10 marks)\n",
        "* Translate the evaluation set using the sequence2sequence attention model (10 marks)\n",
        "* Evaluate the translations made with the sequence2sequence attention model and compare it with the model without attention using BLEU (5 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cddDQ4923cPY"
      },
      "source": [
        "#Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}